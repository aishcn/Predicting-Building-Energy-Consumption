---
title: "STAT 447B Group Report"
author:
  - Anjali Chauhan (57181489)
  - Idris Hedayat ()
  - Sameer Shankar (47555636)
  - Sumit Meghlani ()
date: "March 31, 2022"
output:
  pdf_document: 
    latex_engine: xelatex
  toc_depth: 2
  toc: yes
  extra_dependencies: ["array"]
subtitle: Predicting Building Energy Consumption Using Climate Variables and Building Characteristics

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(png)
library(gridExtra)
```

## 1. Summary

Climate change is an urgent, and multi-faceted issue heavily impacted by infrastructure. Addressing climate change involves mitigation of Greenhouse Gas (GHG) emissions via changes to electricity systems, transportation, buildings, industry, and land use.

According to a report issued by the International Energy Agency, the life cycle of buildings from construction to demolition were responsible for 37% of global energy-related $CO_2$ emissions in 2020. Yet it is possible to drastically reduce the energy consumption of buildings. For example, retrofitted buildings can reduce heating and cooling energy requirements by 50-90 percent.

Accurate predictions of energy consumption can help policymakers target retrofitting efforts; it will determine which buildings have a high site Energy Usage Intensity (EUI), and therefore, which buildings are most suitable for retrofitting (e.g. buildings with high or low temperatures, those that are commercial or residential, and older or newer buildings). Note that Energy Use Intensity is measured in energy per sq. unit area per annum.

The main objective of this study is to analyze differences in building energy efficiency across a variety of buildings and the characteristics around them, from which we build the optimal model(s) to predict the site EUI based on climatic variables and building characteristics. This can help policymakers target retrofitting efforts to maximize emission reductions and thereby cut down on GHG emissions.

By training an ensemble of prediction models including XGBoost, Gradient Boosting Machine, Neural Network, Support Vector Machine and Ensemble Model, we achieved a high prediction performance with an interval score for an ...% prediction interval of ...

(ANJALI/SUMIT/IDRIS) CAN ADD FURTHER DETAILS ABOUT THE METHODS AND THEIR RESULTS OVER HERE.

## 2. Introduction

Site EUI refers to the amount of energy measured per sq. unit area per annum. Understanding site EUI is important for reducing GHG emissions. Hence the study aims to investigate and understand the effect of climate variables and building characteristics on site EUI.

The analysis fits models using different statistical techniques, and aims to find the model with the best 50/80% prediction intervals.

This report summarises all of the primary statistical modelling and analysis results associated with the study. The remainder of this report is organised as follows: Section 3 describes the data collection, provides measurement of the variables and summarises the data. Section 4 presents the data preprocessing and statistical modelling techniques. Section 5 summarises and interprets the results of the statistical analysis conducted. Appendices are provided for further exploratory data analysis along with the code used for the statistical modelling. Section 6 describes the extreme value detection performed on the predicted stream flow values. Lastly, Section 7 presents the limitations and challenges in conducting this analysis.

## 3. Data

### 3.1 Description

The dataset was created in collaboration with Climate Change AI (CCAI) and Lawrence Berkeley National Laboratory (Berkeley Lab). Data contains roughly 100k observations of building energy usage records collected over 7 years and a number of states within the United States. The dataset consists of building characteristics (e.g. floor area, facility type etc), and weather data for the location of the building (e.g. annual average temperature, annual total precipitation etc), as well as the energy usage for the building and the given year (Site EUI). Each row in the data corresponds to the a single building observed in a given year. There are 75757 rows and 64 columns with some missing data.

\begin{center}
\textbf{Table 1}: Description of Variables Used for Analysis
\end{center}

|  | **Variable** |**Unit** | **Description** |
|-|-----|------|:-:|-----------|
| 1. | ***(Response)*** Site EUI | $kBtu$ $ft^{-2}$ | Site Energy Usage Intensity is the amount of heat and electricity consumed by a building as reflected in utility bills |
| 2. | id | Count | Building ID |
| 3. | Year_Factor | Categorical Variable | Anonymized year in which the weather and energy usage factors were observed |
| 4. | State_Factor | Categorical Variable | Anonymized state in which the building is located |
| 5. | Building_Class | Categorical Variable | Building classification |
| 6. | Facility_Type | Categorical Variable | Building usage type |
| 7. | Floor_Area | $ft^2$ | Floor area of the building |
| 8. | Year_Built | Count | Year in which the building was constructed |
| 9. | Energy_Star_Rating | Count | The energy star rating of the building (Score between 1-100 where a higher energy rating means that the building performs better) |
| 10. | Elevation | year | Elevation of the building location |
| 11. | January_Min_Temp  | $°F$ | Minimum temperature in January (in Fahrenheit) at the location of the building |
| 12. | January_Avg_Temp | $°F$ | Average temperature in January (in Fahrenheit) at the location of the building |
| 13. | January_Max_Temp | $°F$ | Maximum temperature in January (in Fahrenheit) at the location of the building |
| 14. | Cooling_Degree_Days | Count | Cooling degree day for a given day is the number of degrees where the daily average temperature exceeds 65 degrees Fahrenheit. Each month is summed to produce an annual total at the location of the building. |
| 15. | Heating_Degree_Days | Count | Heating degree day for a given day is the number of degrees where the daily average temperature falls under 65 degrees Fahrenheit. Each month is summed to produce an annual total at the location of the building. |
| 16. | Precipitation_Inches | $in$ | Annual precipitation in inches at the location of the building |
| 17. | Snowfall_Inches | $in$ | Annual snowfall in inches at the location of the building |
| 18. | Avg_Temp | $°F$ | Average temperature over a year at the location of the building |
| 19. | Days_Below_30F | $°F$ | Total number of days below 30 degrees Fahrenheit at the location of the building |
| 20. | Days_Above_80F | $°F$ | Total number of days above 80 degrees Fahrenheit at the location of the building |
| 21. | Direction_Max_Wind_Speed | $°$ | Wind direction for maximum wind speed at the location of the building. Give in 360-degree compass point directions (e.g. 360 = North, 180 = South, etc.) |
| 22. | Direction_Peak_Wind_Speed | $°$ | Wind direction for peak wind gust speed at the location of the building. Give in 360-degree compass point directions (e.g. 360 = North, 180 = South, etc.) |
| 23. | Max_Wind_Speed | $ms^{-1}$ | Maximum wind speed at the location of the building |
| 24. | Days_With_Fog | Count | Number of days with fog at the location of the building |

\begin{center}
\textbf{Table 2}: Summary Statistics of All Climate Variables
\end{center}
| **Var** | **EVA** | **PEVA** | **SDEN** | **SDEP** | **SWEQ** | **SFAL** | **SMELT** | **TEMP** | **PREC** | **Q** |
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| **Min** | 0.123 | 71.46 | 131.1 | 0.048 | 0.008 | 35.61 | 17.78 | -7.153 | 0.364 | 0.045 |
| **25%** | 0.293 | 1140.49 | 166.9 | 0.213 | 0.044 | 96.80 | 83.57 | -1.417 | 0.684 | 0.599 |
| **50%** | 0.352 | 1352.98 | 194.5 | 0.319 | 0.077 | 128.13 | 117.0 | 0.190 | 0.768 | 0.899 |
| **Mean** | 0.364 | 1238.07 | 195.0 | 0.376 | 0.097 | 136.60 | 127.2 | 0.135 | 0.779 | 0.949 |
| **75%** | 0.427 | 1509.39 | 217.5 | 0.507 | 0.133 | 166.81 | 163.3 | 1.806 | 0.872 | 1.233 |
| **Max** | 0.712 | 1999.25 | 290.7 | 1.120 | 0.382 | 332.46 | 333.8 | 7.014 | 1.270 | 2.436 |

### 3.2 Exploratory Data Analysis

In our Exploratory Data Analysis, we aimed to find how individual explanatory individuals relate and behave alongside the response variable Site EUI as well other explanatory variables. Through wrangling of the data aim to suggest potential transformations to the data in hopes of finding behaviors that will improve performances of future models.

#Investigating Relationships with the Response

#Spearman Correlation

In a preliminary attempt of gathering information on covariate relationships with Site EUI, we used Spearman rank correlation coefficients to investigate potential linear relationships with the response. In particular we choose Spearman correlation which is invariant to monotone increasing effect of transforms. We find that energy star rating has by far the largest Spearman correlation with Site EUI in terms of magnitude with -0.654 [REPLACE WITH VALUE FROM SUBSET ??], suggesting a relatively strong decreasing relationship with the response. We also note relatively higher magnitudes for monthly min and average temperatures, in particular a decreasing relationship with colder months temperature measures, namely January and February, while there are increasing relationships with Summer months auch as June and July. In context this can be expected as there is a lesser requirement of energy for uses such as heating and lighting in building in the summer months when days last longer and are warmer.


#Plots Investigating relationships with Site EUI

In order to enable us to explore the data more effectively there are measures that can be taken such as subsetting the data, given the large size of the data set.
Via the summary tables we also see that site EUI is right skewed, so a cube root transformation for the sole purpose of investigating relationship will be more effective.

#building charactersitics vs site eui

building characteristics are of particular interest in context of the data, and so we investigated relationships between Site EUI and building variables. The most noteworthy relationships was with energy star rating, which was as expected from the Spearman correlations, and found a clearer and more linear relationship with the response than almost all other variables.

For elevation we used binning for the classes based on the variables quantiles from summary statistics. We see a notable increase in site EUI initially as buildings add a floors requirement more total energy usage, before this plateaus, which is understandable in context as buildings aren't generally built above a certain height and number of floors.

For state factor we see that the classes are very imbalanced for Stat 10 with only 15 datapoints, while stat 6 has much more than the others with 50840, indicating we could undersample for this specific state factor, while combining stat 10 with another state factor level.

#Investigating Relationships between explanatory variables

It is in our interest to explore potential relationships between covariates in the dataset, as these could potentially lead to issues namely multicollinearity.
The monthly minimum, maximum, and avaerge temperatures for colder months display relatively strong relationships with other climatic variables relating to cooler weather including "days below...", "heating degree days" and "snowfall" covariates. We gather that the strong negative relationship in context shows as temperature increases in these cold months, the amount of snow fall and days below 0F and 20F decrease as expected. Tther noteworthy relationship

```{r echo=FALSE, out.width='70%', fig.align="center"}
knitr::include_graphics('image/all_climate_var_scatterplots.png')
```

\begin{center}
\textbf{Fig. 1}: Relationship between Climate Variables and Streamflow
\end{center}


```{=latex}
\begin{center}
\begin{tabular}{>{\centering}p{8cm}|>{\centering}p{8cm}}
\includegraphics[width=80mm, height=70mm]{image/unscaled_boxplot.png} & \includegraphics[width=80mm, height=70mm]{image/all_climate_var_scaled_boxplots.png} \\
\end{tabular}
\begin{tabular}{>{\centering}p{8cm}|>{\centering}p{8cm}}
\textbf{Fig. 2}: Before Scaling & \textbf{Fig. 3}: After Scaling \\
\end{tabular}
\end{center}
```


```{=latex}
\begin{center}
\begin{tabular}{>{\centering}p{8cm}|>{\centering}p{8cm}}
\includegraphics[width=80mm, height=70mm]{image/barchart_gridcode.png} & \includegraphics[width=80mm, height=70mm]{image/peva_gridcode.png} \\
\end{tabular}
\begin{tabular}{>{\centering}p{8cm}|>{\centering}p{8cm}}
\textbf{Fig. 4}: Numbers of Observation by Gridcode & \textbf{Fig. 5}: Stream Flow vs Mean Potential Evaporation for Selected Gridcodes \\
\end{tabular}
\end{center}
```

Looking more closely at various variables broken out by `gridcode` (*see Fig. 5 as an example*), some interesting results were seen. For many `gridcode`'s, there appears to be a much stronger linear relationship between climate variables and the stream flow and these relationships appear to have different intercept and slope values. This suggests that there may not be a one-size-fits-all approach to fitting a regression model based on annual climate variables alone and different slopes for different `gridcode`'s may need to be considered for further analysis in the future.

## 4. Methods

### 4.0. Pipeline

Below in Fig. 6, we have a Proof-of-Concept pipeline that addresses all of the client's research questions. A breakdown of each of the steps shown in the end-to-end workflow diagram is covered below. 


```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('image/new_pipeline.png')
```

\begin{center}
\textbf{Fig. 6}: End-to-End Pipeline
\end{center}


### 4.1 Data Pre-Processing

As part of the data pre-processing pipeline, we start by dropping duplicate rows, and columns with a high percentage of missing values (over 50%). With the remaining missing values, we used the median imputation method (filling missing values of the column with the median). Furthermore, we dropped highly correlated features using pairwise correlation analysis, and binned the facility type column (due to unbalanced classes). Lastly, we removed rows with extreme Site EUI values using the IQR method and we used an ordinal encoding to encode all categorical columns.

#### 4.1.1 Feature Transformations
\
\

In addition to Z-Score Standardization, which refers to scaling and centering of the distribution to ensure the data is not on a varying scale and is internally consistent, several feature transformations were performed to deal with both left and right skewed feature distributions. Table 3 shows all the transformations performed in order to achieve the desired results which are highlighted by Fig. 7 and Fig. 8.

\begin{center}
\textbf{Table 3}: Feature Transformations
\end{center}
| **Variable** | **Description** | **Transformation** | **Skewness** |
|--|----|----|--|
| `EVA` | Mean Yearly Evaporation | Cube Root Transform | Right Skewed |
| `SDEN` | Mean Snow Density | Square Root Transform | Right Skewed |
| `SDEP` | Mean Snow Depth | Log Transform | Right Skewed |
| `SWEQ` | Mean Snow Depth of Snow Water Equivalent | Log Transform | Right Skewed |
| `SFAL` | Mean Yearly Snowfall | Square Root Transform | Right Skewed |
| `SMELT` | Mean Yearly Snowmelt | Square Root Transform | Right Skewed |
| `PEVA` | Mean Yearly Potential Evaporation | Shifted Square Root Transform | Left Skewed |


```{=latex}
\begin{center}
\begin{tabular}{>{\centering}p{8cm}|>{\centering}p{8cm}}
\includegraphics[width=80mm, height=70mm]{image/density_plot.png} & \includegraphics[width=80mm, height=70mm]{image/afterTransformation.png} \\
\end{tabular}
\begin{tabular}{>{\centering}p{8cm}|>{\centering}p{8cm}}
\textbf{Fig 7}: Before Transformation & \textbf{Fig 8}: After Transformation \\
\end{tabular}
\end{center}
```


### 4.2 Feature Selection

One of the most important step in our pipeline was Feature Selection as it is one of the main objectives. We have used several different methods in order to achieve this objective:

#### 4.2.1. Pairwise Correlation Analysis\
\
Before using any of the traditional feature selection techniques mentioned below, we investigated if any of the features were highly correlated. We see from the Correlation Matrix below that there is high correlation for monthly temperature averages particularly those around the sames seasons suggesting future implementation of new features in the place of seperate months. We also see high correlation for other climatic features such as days above and below variable, as well as high correlation for snowfall and precipitation inches, also indicating potential interactions or new replacement covariates in place of these terms

```
knitr::include_graphics('image/corr.png')
# plt.figure(figsize=(16, 6))
# # define the mask to set the values in the upper triangle to True
# mask = np.triu(np.ones_like(df.corr(), dtype=np.bool))
# heatmap = sns.heatmap(df.corr(), mask=mask, vmin=-1, vmax=1, annot=False, cmap='BrBG')
# heatmap.set_title('Triangle Correlation Heatmap', fontdict={'fontsize':10}, pad=16)

\begin{center}
\textbf{Fig. 9}: Heatmap Presenting Correlation Between Variables
\end{center}
```


#### 4.2.2. Variable Importance using boruta package\
\

This method is built on a random forest classifier. It ranks features based on their importance measure i.e. Mean Decrease Accuracy (MDA) where higher means more important. MDA measures how much accuracy the model losses by excluding each variable. The more the accuracy degrades, the more important the variable is. 
From the variable importance plot we see the 3 most importance variables by far are energy star rating floor area and year built. We proposed the notion of  potential interactions with these terms. [NOT SURE EXACTLY ON REASONING FOR THE INTERACTION LATER ??]. We also note particular importance of Facility Type, Elevation, Days below 20F, Ferbruary average temperature, January average temperature, as well as building class, relative  to the other features in the dataset.


knitr::include_graphics('image/new_borota.png')
# from sklearn.ensemble import RandomForestRegressor
# rf = RandomForestRegressor(n_estimators=80, max_features='auto')
# rf.fit(df.iloc[:,:-1], df['site_eui'])
# print("Training done using Random Forest")
# 
# ranking = np.argsort(-rf.feature_importances_)
# f, ax = plt.subplots(figsize=(11,9))
# sns.barplot(x=rf.feature_importances_[ranking], y=df.iloc[:,:-1].columns.values[ranking], orient='h')
# ax.set_xlabel("Feature Importance")
# plt.tight_layout()
# plt.show()

\begin{center}
\textbf{Fig 12}: Variable Importance (using 'boruta' package)
\end{center}

#### 4.2.3. Interactions and Additional features
Based on the variable importance and pairwise correlations we decided on adding interaction terms for future models. We decided on including interactions of; 
- floorarea and year built
- floor area and energy star rating
- all 3; floor area, year built, and energy star rating
- floor area and sum of Cooling Heating degree Days

We also implemented new features containing other features that were otherwise correlated with one another, notably monthyl temperature variabels. Thus we decided to introduce seasonal terms, where Spring took the average of March April May temperatures, Summer took average of Juny July August, Fall took average of September October November, and Winter took December January and February.

We also introduced terms for "days below.." and "days above.." temperature variables;
- Freezing days: total days below 0 10 F
- Cold days:  total days below 30 and 20 F
- Warm days: total days above 80 and 90 F
- Hot days: days above 100 and 110 F

Along with a feature covering both snowfall and precipiation:
- Snow Rain inches: sum total inches of snowfall and precipitation

In adding these features we removed the originals contained within these, that in turn would help leave is with more parsimonious models to interpret.

#### 4.2.4 Feature Transformations
\
\
In addition to Z-Score Standardization, which refers to scaling and centering of the distribution to ensure the data is not on a varying scale and is internally consistent, several feature transformations were performed to deal with both left and right skewed feature distributions. Table 3 shows all the transformations performed in order to achieve the desired results which are highlighted by Fig. 7 and Fig. 8.

\begin{center}
\textbf{Table 3}: Feature Transformations
\end{center}
| *Variable* | *Description* | *Transformation* | *Skewness* |
|--|----|----|--|
| floor_area | Floor Area | Fifth Root Transform | Right Skewed |
| year_buily | Year Built | Shifted (1200 - year built) & Squared Transform | Left Skewed |
| energy_star_Rating | Energy Star Rating  | Squared Transform | Left Skewed |
| ELEVATION | Building Elevation | Sixth Root Transform | Right Skewed |
| avg_temp | Average Temperature | None | None Obvious |
| SpringTemp | Average Temperatures of March April May | None | None Obvious |
| SummerTemp | Average Temperatures of Juny July August | None | None Obvious |
| FallTemp | Average Temperatures of September October November | None | None Obvious |
| WinterTemp | Average Temperatures of December January February | None | None Obvious |
| floorxBuilt | Interaction Term Floor Area and Year Built  | Sixth Root Transform | Left Skewed |
| floorxEnergy | Interaction Term Floor Area and Energy Star Rating  | Sixth Root Transform | Left Skewed |
| floorxBuilt | Interaction Term Floor Area and Year Built  | Sixth Root Transform | Right Skewed |
| floorxHeatCool | Interaction Term Floor Area and Sum of Heating and Cooling Degree Days  | Sixth Root Transform | Right Skewed |
| freezing_days | Total days below 0 and 10 F | Fifth Root Transform | Slight Right Skewed |
| cold_days | Total days below 20 and 30 F | square Root Transform | Slight Right Skewed |
| warm_days | Total days above 80 90 F | Square Root Transform | Left Skewed |
| hot_days | Total days above 100 110 F | Dropped | Too Few Data |

### 4.3. Model Training and Validation

A good variety of models were implemented as a part of our analysis for extensive results. Two sets of models where trained to capture both the effect of individual co-variate terms and the interaction terms on the predictive performance of the model. These models were cross-validated (10-fold, Repeated CV) and their hyper parameters were fine tuned using Random Search. Please refer to Table 3 and Table 4 for results that denote the predictive performance of the models. 

#### 4.3.1. Linear Models\
\
After feature selection, we trained a Linear Regression model with a 10-fold cross validation to test the model performance on the training data. We train a similar set of models with interaction terms as features and we see slightly better results than the former method.

#### 4.3.2. Support Vector Machines\
\
To add further complexities to the previous models, we trained Support Vector Machines that expanded our feature space using different kernels. We have a radial kernel to compare performance of models without the complexities relating to Linearity respectively. In the radial  kernel, only the neighboring behaviour of data is taken into account which means only those data points influence the modelling compared to the Linear SVM whose performance is similar to a Linear model. From Table 3 and Table 4, we see that the Radial SVM without interaction terms performs slightly better than Radial SVM with the interaction terms.	

#### 4.3.3. Tree Models\
\
We used three tree-based models such as Random Forest, XGBoost, Quantile Regression Forest (QRF) and Gradient Boosting Machine to improve the performance compared to the above models. From Table 3 and Table 4, we see that the Random Forest, QRF and XGBoost with interaction terms has better results compared to without interaction terms. However, the GBM without interaction terms gives slightly better results than the GBM with interaction terms. The best tree model is the Quantile Regression Forest Model with the lowest RMSE of 0.227 (with interaction terms).

\
\

#### 4.3.4. Ensemble Models\
\
We trained two Ensemble models with/without the interaction terms that combine the above listed 6 models to produce improved results. These models generally produce more accurate predictions than a single model. From Table 3 and Table 4, we see that that the Ensemble Model with interaction terms has significantly better results compared to the Ensemble Model without interaction terms. Therefore, based on the validation evaluation metrics from Table 3 and 4, we chose the Ensemble Model with the interaction terms as our best model.

<!---
Fig. 10: Comparing In-sample Prediction Performance for Different Models without Interaction Terms   |  Fig. 11: Comparing In-sample Prediction Performance for Different Models with Interaction Terms 
:-------------------------:|:-------------------------:
![](image/compare_no_int.png)  |  ![](image/compare.png)
--->

```{=latex}
\begin{center}
\begin{tabular}{>{\centering}p{8cm}|>{\centering}p{8cm}}
\includegraphics[width=80mm, height=70mm]{image/Evaluation_Metrics_without_Interaction_terms.png} & \includegraphics[width=80mm, height=70mm]{image/Evaluation_Metrics_with_Interaction_terms.png} \\ 
\end{tabular}
\begin{tabular}{>{\centering}p{8cm}|>{\centering}p{8cm}}
\textbf{Fig. 15}: Comparing In-sample Prediction Performance for Different Models Without Interaction Terms & \textbf{Fig. 16}: Comparing In-sample Prediction Performance for Different Models With Interaction Terms \\
\end{tabular}
\end{center}
```

```{r echo=FALSE, out.width='60%', fig.align="center"}
knitr::include_graphics('image/new_train_result.png')
```

\begin{center}
\textbf{Fig. 17}: Comparing Prediction Performance for Different Models Without and With Interaction Terms
\end{center}

\
\
\
\
\
\
\
\

## 5. Model Results

From Table 3, we see that the ensemble model have the lowest RMSE , meaning it has the best predictive performance out of all the models with no interaction terms. 

\begin{center}
\textbf{Table 4}: Comparing Prediction Performance of Different Models Without the Interaction Terms
\end{center}

| **Models** | **RMSE** |
|---|:---:|
| Linear Model | 0.203 | 
| Quantile Regression Forest | 0.256 | 
| Random Forest | 0.255 |
| XGBoost Linear | 0.255 |
| Radial Support Vector Machine | 0.218 | 
| Gradient Boosting Machine | 0.262 | 
| **Ensemble Model** | **0.1986** |

From Table 4, we see that once again that the ensemble of all the 6 models listed has the lowest RMSE, meaning it has the best predictive performance out of all the individual models with interaction terms. 

\begin{center}
\textbf{Table 5}: Comparing Prediction Performance of Different Models With the Interaction Terms
\end{center}

| **Models** | **RMSE** |
|---|:---:|
| Linear Model | 0.196 | 
| Quantile Regression Forest | 0.227 | 
| Random Forest | 0.234 |
| XGBoost Linear | 0.242 |
| Radial Support Vector Machine | 0.267 | 
| Gradient Boosting Machine | 0.274 | 
| **Ensemble Model** | **0.1878** |

From Table 5, we use our best performed models on test dataset to evaluate whether our models are still valid when applying on test dataset, we can see that the RMSE from both models are quqite consistent with their result in Training dataset in Table 3 and 4. 


\begin{center}
\textbf{Table 6}: Prediction Performance of Best Models on Testing Dataset
\end{center}

| **Models** | **RMSE** |
|---|:---:|
| Ensemble Model Without Interaction Terms | 0.192664 |
| Ensemble Model With Interaction Terms | 0.2042341 |

<!---
Fig. 12: Predicted vs Actual Values (Ensemble Model)   |  Fig. 13: Predicted vs Actual Values (GLM) | Fig 11: Predicted vs Actual Values (Linear Regression)
:-------------------------:|:-------------------------:|:-------------------------:
![](image/best.png)  |  ![](image/glm.png) | ![](image/lm.png) |
--->

The two plots below show how much the prediction from our best 2 models deviated from actual value therefore giving us a rough estimation of whether a model is a good fit or not. We clearly see that both models'prediction are quite near the actual values, most points were near to the fitted line indicating a good fit.


```{=latex}
\begin{center}
\begin{tabular}{>{\centering}p{8cm}|>{\centering}p{8cm}}
\includegraphics[width=8cm, height=40mm]{image/with int residual plot.png} & \includegraphics[width=8cm, height=40mm]{image/without int residual plot.png}\\ 
\end{tabular}
\begin{tabular}{>{\centering}p{8cm}|>{\centering}p{8cm}}
\textbf{Fig. 18}: Predicted vs Actual Values (Ensemble Model with interaction terms) & \textbf{Fig. 19}: Predicted vs Actual Values (Ensemble Model without interaction terms)  \\
\end{tabular}
\end{center}
```

Comparing the predictive performance of the best models from both Table 3 and Table 4, we see that the latter (Ensemble Model, RMSE: 0.1795) has a better performance. Therefore, the best model we chose for making predictions was the Ensemble Model with the interaction terms. 

## 6. Outlier Detection


We examined the stream flow values in our dataset by using the interquartile range (IQR) method, classifying any stream flow values more than 1.5 times the IQR below the first quartile or above the third quartile as an outlier. Looking at Fig. 14, there are 16 extreme high points that are outliers but no extreme low points. Grouping stream flow values by gridcodes is looking at each watershed separately. The average stream flow values vary from watershed to watershed. The average stream flow values for watershed at grid code 14 is higher than the rest of the watersheds. On the other hand, the average stream flow values for watershed at grid code 1264 is lower than the rest of the locations. Gridcodes 1212 and 1391 have the most outliers and it is worth looking into and learning more about those areas. 


```{r echo=FALSE, out.width='50%', fig.align="center"}
knitr::include_graphics('image/outliers_by_gridcode.png')
```

\begin{center}
\textbf{Fig. 20}: Streamflow Outliers vs Gridcode
\end{center}

\
\
\
\


## 7. Limitations


- After feature selection, there is some potential that the best features for each model type were not selected. When selecting the features that would be included in the hyper-parameter tuning for the models we used the results of both Boruta (random forest based selection method) and Forward Stepwise Regression (regression based selection method). The selected features from each both agreed with each other so they seemed reliable, however we did not do an exhaustive search over all variables for each of the models and it is possible there were better combinations. 

- Gridcode being selected as an important feature may lead to poor model performance due to lack of data to properly fit especially in the ‘with interaction’ case. There are 23 separate gridcodes, so there are only 20-40 observations for each gridcode which is not very much (especially for tree based models). Having access to more data could result in much higher performance.

- Each observation in the dataset is an aggregation of climate data collected over the year (i.e. they are the average values collected over the year). This limits the forecasting power of the predictive models that we have fit as we would need to use the forecasted explanatory variables to predict the stream flow, which will most likely lower the performance of the model.


## 8. Conclusion

After all the model iterations and improvements, we were able to achieve fairly good results with the Ensemble Model taking into account the interaction effect between variables. These results have large implications when it comes to water resource management economically. We have conducted our primary analysis taking into account the spatial data (e.g. gridcodes) which serves as a good MVP to predict the streamflow. 

As a side interest and to expand upon the idea of predicting the streamflow solely using climate variables and not any spatial and temporal data, we conducted an analysis and trained models without this data and the predictive performance dropped significantly. Although this addresses the client’s first research question of whether one catchment can be used to extrapolate stream flow in another catchment, the limitation we faced was lack of training data. To be able to model such a complex problem using climatic variables, we need more data to train our model which can help with improving the predictive performance of the model. 

To address the second research question of whether or not we can detect the unusual streamflow activity accurately, we built a proof of concept pipeline for the outlier detection system. It will take the features from our prediction model as input and label the observations as either anomalous or regular depending on the anomaly scores which are the measures of deviation from normal behavior. We will face the same challenge here - lack of training data. This will lead to an increase in false positives and false negatives in the outlier detection system which can have detrimental consequences. For example, not being able to detect a subtle increase in the streamflow (false negatives) which could lead to irrigation problems and in severe cases even floods. Or getting a huge pool of outlier values (false positives) that will raise false alarms of anomalous behavior more often than desired. 


## 9. Future Research

There are two areas of further research that would help address the limitations in this study; first is further investigating outliers and anomaly detection and second is training a model on a more granular time frame. 

- The outlier detection that we have done is appropriate for identifying outliers in the current dataset but does not have a real-world application.  Training a classification model to predict extreme values in streamflow and/or training an anomaly detection model to find unusual patterns in streamflow and the climate variables. Both models could have more real-world application in flood/drought prevention which is useful in fields such as agriculture. 

- Additionally, requiring input data for the whole year for the model makes it impractical to accurately predict streamflow values in the future as we would need to use forecasted values for the input variables. Having the data be on a more granular time frame would greatly benefit the analysis and real-world predictive power of the model as we could train the predictive streamflow model only using past variables. The resulting model would not rely on using forecasted input variables, which addresses a critical limitation of this study. 


## 10. References

- Government of Canada / Gouvernement du Canada. (2021, November 25). Government of Canada / gouvernement du Canada. Climate. Retrieved February 5, 2022, from https://climate.weather.gc.ca/glossary_e.html 
- US Department of Commerce, N. O. A. A. (2012, March 8). Snow measurement guidelines. Snow Measurement Guidelines. Retrieved February 5, 2022, from https://www.weather.gov/gsp/snow 
- Janssen, J., & Ameli, A. A. (2021). A Hydrologic Functional Approach for Improving Large‐Sample Hydrology Performance in Poorly Gauged Regions. Water Resources Research, 57(9), e2021WR030263.
- Statistical interaction: More than the sum of its parts. Statistics Solutions. (2021, June 22). Retrieved February 21, 2022, from https://www.statisticssolutions.com/statistical-interaction-more-than- the-sum-of-its-parts/



## 11. Appendix I (For Client)



## 12. Appendix II (For Mentor)

## 12.1. Installing Packages

```{r lib, message=FALSE, warning=FALSE}
#Sys.setenv(LANG = "en")
# library(tidyverse)
# library(ggplot2)
# library(GGally)
# library(DataExplorer)
# library(olsrr)
# library(gridExtra)
# library(cluster)
# library(factoextra)
# library(caretEnsemble)
# library(caret)
# library(mlbench)
# library(Metrics)
# library(Boruta)
# library(tidymodels)   # packages for modeling and statistical analysis
# library(tune)         # For hyperparemeter tuning
# library(workflows)    # streamline process
# library(tictoc)
# library(quantregForest)
# library(e1071)
# library(solitude) 
# library(RColorBrewer)
```

## 12.2. Loading the Data

```{r 1.1, echo=TRUE, message=FALSE, warning=FALSE}

# Loading the data
# dt = read_csv('data_stat450.csv')
# 
# dt2 = dt

# # Removing 'year' column
# dt = dt[-c(2)]
# 
# # Renaming column names for simplicity
# colnames(dt) = c('gridcode','EVA','PEVA', 'SDEN','SDEP','SWEQ','SFAL','SMELT','TEMP',
#                  'PREC','Q')
# colnames(dt2) = c('gridcode','year','EVA','PEVA', 'SDEN','SDEP','SWEQ','SFAL','SMELT'
#                       ,'TEMP', 'PREC','Q') # for anomaly detection
# 
# # Converting gridcode to a factor 
# dt$gridcode = as.factor(dt$gridcode)
# 
# head(dt)
# length(unique(dt$gridcode)) # 23
```

## 12.3. Explanatory Data Analysis

```{r 1.2}
# Generating summary statistics for dt
# summary(dt)
# 
# # EDA
# plot_intro(dt)
# plot_missing(dt)
# plot_bar(dt)
# plot_histogram(dt)
# plot_density(dt)
# plot_qq(dt)
# plot_qq(dt, by = "gridcode")
# plot_correlation(dt)
# plot_boxplot(dt, by = "gridcode")
# plot_scatterplot(split_columns(dt)$continuous, by = "Q")
# plot_prcomp(na.omit(dt), maxcat = 4L)
# 
# # Checking dimensions of dt
# nrow(dt) # 774
# ncol(dt) # 11

```

## 12.3.1. Examining Variables by Gridcode
```{r vars_by_gridcode, eval=FALSE}
# look at Stream Flow vs Mean Potential Evaporation
# set.seed(123)
# dt.valid <- na.omit(dt)
# gridcode_sample <- dt.valid |> select(gridcode) |> distinct()
# gridcode_sample <- sample(gridcode_sample$gridcode, 9)
# dt.valid |> 
#     filter(gridcode %in% gridcode_sample) |>
#     ggplot(aes_string(x="PEVA", y="Q")) +
#     geom_point() +
#     facet_wrap(~gridcode) +
#     geom_smooth(method="lm", se=FALSE, formula=y~x) +
#     labs(title="Stream Flow vs Mean Potential Evaporation") +
#   ylab('Stream Flow') +
#   xlab('Mean Potential Evaporation') + 
#   theme(plot.caption = element_text(hjust = 0))
```
### 12.4. Data Preprocessing
```{r 2.1.1, eval=FALSE, warning=FALSE}
# labels <- paste(colnames(dt[,2:11]))
# boxplot(dt[,2:11],  xaxt = "n",xlab = "",
#         main = 'Comparing different explanatory variables (before scaling)')
# axis(1, labels = FALSE)
# text(x =  seq_along(labels), y = par("usr")[3] - 1, srt = 60, adj = 1,
#      labels = labels, xpd = TRUE)
```
#### 12.4.1. Transformations
```{r}
# Transformations

## right-skewed
#dt$Q = sqrt(dt$Q)
# dt$EVA = (dt$EVA)^(1/3) 
# dt$SDEN = sqrt(dt$SDEN)
# dt$SDEP = log(dt$SDEP)
# dt$SWEQ = log(dt$SWEQ)
# dt$SFAL = sqrt(dt$SFAL)
# dt$SMELT = sqrt(dt$SMELT)
# 
# # left-skewed
# dt$PEVA = sqrt(2000-dt$PEVA) 
# 
# plot_density(dt|> select(EVA,SDEN,SDEP, SWEQ, SFAL, SMELT,PEVA))
```
#### 12.4.2. Preprocessing Pipeline

```{r 2.1.2, warning=FALSE}
# set.seed(2020)
# rec <- recipe(Q ~., 
#               data = dt[,2:11]) %>%  
#   step_corr(all_predictors()) %>%                  # removing highly correlated features
#   # Z-Score Standardization
#   step_center(all_numeric(), -all_outcomes())%>%   # centering data at mean = 0
#   step_scale(all_numeric(), -all_outcomes())       # scaling data with variance  = 1
# 
# trained_rec =  prep(rec, training = dt, retain = TRUE)
# dt_prep = cbind(gridcode = dt$gridcode, as.data.frame(juice(trained_rec)))
# 
# # Separating the actual test set w/o labels
# main_dt <- na.omit(dt_prep)
# test_nolabel_df <- dt_prep[is.na(dt$Q),]
# 
# main_dt
# 
# labels <- paste(colnames(main_dt[,2:8]))
# boxplot(main_dt[,2:8],  xaxt = "n",xlab = "")
# axis(1, labels = FALSE)
# text(x =  seq_along(labels), y = par("usr")[3] - 1, srt = 60, adj = 1,
#      labels = labels, xpd = TRUE)
```

#### 12.4.3. Relationship b/w explanatory variables & the response
```{r}
# plot_scatterplot(split_columns(main_dt)$continuous, by = "Q")
# 
# ggpairs(main_dt[,2:8],lower = list(continuous = wrap("smooth", alpha = 0.3, 
#                                                   size=0.05)),
#       upper = list(continuous = wrap("cor", size=2)))
```

#### 12.4.4. Feature Selection Using `boruta`

```{r}
# boruta_output <- Boruta(Q ~ ., data=na.omit(main_dt), doTrace=0)
# roughFixMod <- TentativeRoughFix(boruta_output)
# boruta_signif <- getSelectedAttributes(roughFixMod)
# 
# # Variable Importance Scores
# imps <- attStats(roughFixMod)
# imps2 = imps[imps$decision != 'Rejected', c('meanImp', 'decision')]
# head(imps2[order(-imps2$meanImp), ])  # descending sort
# 
# # Plot variable importance
# plot(boruta_output, cex.axis=.7, las=2, xlab="", ylab = "Variable Importance")
# 
# selected_features <- c('PREC','gridcode','EVA','TEMP','PEVA','SFAL','SDEN')
# 
# main_fs <- main_dt[,(colnames(main_dt) %in% append(selected_features, "Q"))]
# main_fs
```


#### 12.4.5. Feature Selection Using Forward Stepwise Regression

```{r}
# dt_interaction <- main_dt
# FS.lm <- lm(Q ~ (.)^2, data = dt_interaction)
# 
# OLS <- ols_step_forward_p(FS.lm)
# OLS
# plot(OLS)
```
### 12.5. Model Training

#### 12.5.1. Splitting Data into Train/Test sets

```{r 5.1}
# set.seed(123)
# split <- createDataPartition(y=main_fs$Q, p=.8,list=F)
# train <- main_fs[split,]
# nrow(train)
# test <- main_fs[-split,]
# nrow(test)
```

#### 12.5.2. Cross-Validation (w/o Interaction terms)
```{r}
# set.seed(123)
# my_control = trainControl(method = 'repeatedcv', # for “cross-validation”,
#                            repeats = 3,
#                            number = 10, # number of k-folds
#                            savePredictions = 'final',
#                            search = 'random')
# 
# model_list1 = caretList(Q~.,
#                         data = train,
#                         methodList = c('lm', 'rf', 'qrf', 'xgbLinear','svmRadial', 
#                                        'gbm'),
#                         tuneList = NULL)
# 
# ensemble1 = caretEnsemble(model_list1, 
#                           metric = 'RMSE', 
#                           trControl = my_control)

```

#### 12.5.3 Cross-Validation (w/ Interaction terms)
```{r}
# model_list2 = caretList(Q ~ gridcode+PREC+SDEN+EVA+gridcode:SFAL+TEMP+PEVA+SFAL+
#                          TEMP:PREC+SDEN:SFAL+
#                          SFAL:TEMP+SDEN:PREC+EVA:TEMP+gridcode:PEVA+
#                          PEVA:TEMP+gridcode:PREC+gridcode:EVA+
#                          EVA:SDEN+gridcode:SDEN+SDEN:TEMP,
#                 data = train,
#                         trControl = my_control,
#                         methodList = c('lm', 'rf', 'qrf', 'xgbLinear', 
#                         'svmRadial', 'gbm'),
#                         tuneList = NULL)
# 
# ensemble2 = caretEnsemble(model_list2, 
#                           metric = 'RMSE', 
#                           trControl = my_control)
```

#### 12.5.4 Evaluation Metrics (w/o Interaction terms)
```{r}
# options(digits = 3)
# model_results1 = data.frame(
#   LM = mean(model_list1$lm$results$RMSE),
#   QRF = mean(model_list1$qrf$results$RMSE),
#   RF = mean(model_list1$rf$results$RMSE),
#   XGBL = mean(model_list1$xgbLinear$results$RMSE),
#   SVMR = mean(model_list1$svmRadial$results$RMSE),
#   GBM = mean(model_list1$gbm$results$RMSE)
# )
# 
# best_model_train1 = apply(model_results1, 1, FUN = mean)
# print(model_results1)
# 
# resamples1 <- resamples(model_list1)
# resamples1
# summary(resamples1)
# dotplot(resamples1, metric = 'RMSE')
# modelCor(resamples1)
# 
# # Ensemble Model Results
# summary(ensemble1)
# plot(ensemble1)
# 
# scales1 = list(x=list(relation='free'), y=list(relation='free'))
# bwplot(resamples1,scales = scales1,layout = c(2,2))
```

#### 12.5.5 Evaluation Metrics (w/ Interaction terms)

```{r}
# options(digits = 3)
# model_results2 = data.frame(
#   LM = mean(model_list2$lm$results$RMSE),
#   QRF = mean(model_list2$qrf$results$RMSE),
#   RF = mean(model_list2$rf$results$RMSE),
#   XGBL = mean(model_list2$xgbLinear$results$RMSE),
#   SVMR = mean(model_list2$svmRadial$results$RMSE),
#   GBM = mean(model_list2$gbm$results$RMSE)
# )
# 
# best_model_train2 = apply(model_results2, 1, FUN = mean)
# print(model_results2)
# 
# resamples2 <- resamples(model_list2)
# resamples2
# summary(resamples2)
# dotplot(resamples2, metric = 'RMSE')
# modelCor(resamples2)
# 
# # Ensemble Model Results
# summary(ensemble2)
# plot(ensemble2)
# 
# scales2 = list(x=list(relation='free'), y=list(relation='free'))
# bwplot(resamples2,scales = scales2,layout = c(2,2))
```

### 12.6 Predictions 

#### 12.6.1. Predictions (w/o Interaction terms)
```{r}

# # PREDICTIONS
# pred_lm1 <- predict.train(model_list1$lm, newdata = test)
# pred_qrf1 <- predict.train(model_list1$qrf, newdata = test)
# pred_rf1 <- predict.train(model_list1$rf, newdata = test)
# pred_xgbL1 <- predict.train(model_list1$xgbLinear, newdata = test)
# pred_svmr1 <- predict.train(model_list1$svmRadial, newdata = test)
# pred_gbm1 <- predict.train(model_list1$gbm, newdata = test)
# predict_ens1 <- predict(ensemble1, newdata = test)
# 
# # RMSE
# y_test = test[,8]
# pred_RMSE1 <- data.frame(ENS = RMSE(predict_ens1, y_test),
#                         LM = RMSE(pred_lm1, y_test),
#                         QRF = RMSE(pred_qrf1, y_test),
#                         RF = RMSE(pred_rf1, y_test),
#                         XGBL = RMSE(pred_xgbL1, y_test),
#                         SVMR = RMSE(pred_svmr1, y_test),
#                         GBM = RMSE(pred_svmr1, y_test))
#                         
# print(pred_RMSE1)
# 
# best_model_test1 = apply(pred_RMSE1, 1, FUN = mean)
# 
# pred_cor1 <- data.frame(ENS = cor(predict_ens1, y_test),
#                        LM = cor(pred_lm1, y_test),
#                        QRF = cor(pred_qrf1, y_test),
#                        RF = cor(pred_rf1, y_test),
#                        XGBL = cor(pred_xgbL1, y_test),
#                        SVMR = cor(pred_svmr1, y_test),
#                        GBM = cor(pred_gbm1, y_test),
#                        ENS = cor(predict_ens1, y_test))
# 
# print(pred_cor1)
# ```
# ```{r}
# # par(mfrow = c(3,3))
# # plot(pred_lm1, y_test) + abline(0,1, col = 'red')
# # plot(pred_qrf1, y_test) + abline(0,1, col = 'red')
# # plot(pred_rf1, y_test) + abline(0,1, col = 'red')
# # plot(pred_xgbL1, y_test) + abline(0,1, col = 'red')
# # plot(pred_svmr1, y_test) + abline(0,1, col = 'red')
# # plot(pred_gbm1, y_test) + abline(0,1, col = 'red')
# plot(predict_ens1, y_test, xlab="Prediction",ylab="Actual") +
#     abline(0,1, col = 'red')
```

#### 12.6.2. Predictions (w/ Interaction terms)

```{r}

# # PREDICTIONS
# pred_lm2 <- predict.train(model_list2$lm, newdata = test)
# pred_qrf2 <- predict.train(model_list2$qrf, newdata = test)
# pred_rf2 <- predict.train(model_list2$rf, newdata = test)
# pred_xgbL2 <- predict.train(model_list2$xgbLinear, newdata = test)
# pred_svmr2 <- predict.train(model_list2$svmRadial, newdata = test)
# pred_gbm2 <- predict.train(model_list2$gbm, newdata = test)
# predict_ens2 <- predict(ensemble2, newdata = test)
# 
# # RMSE
# y_test = test[,8]
# pred_RMSE2 <- data.frame(ENS = RMSE(predict_ens2, y_test),
#                         LM = RMSE(pred_lm2, y_test),
#                         QRF = RMSE(pred_qrf2, y_test),
#                         RF = RMSE(pred_rf2, y_test),
#                         XGBL = RMSE(pred_xgbL2, y_test),
#                         SVMR = RMSE(pred_svmr2, y_test),
#                         GBM = RMSE(pred_gbm2, y_test))
#                         
# print(pred_RMSE2)
# 
# best_model_test2 = apply(pred_RMSE2, 1, FUN = mean)
# 
# pred_cor2 <- data.frame(ENS = cor(predict_ens2, y_test),
#                        LM = cor(pred_lm2, y_test),
#                        QRF = cor(pred_qrf2, y_test),
#                        RF = cor(pred_rf2, y_test),
#                        XGBL = cor(pred_xgbL2, y_test),
#                        SVMR = cor(pred_svmr2, y_test),
#                        GBM = cor(pred_gbm2, y_test),
#                        ENS = cor(predict_ens2, y_test))
# 
# print(pred_cor2)
# ```
# ```{r}
# # par(mfrow = c(3,3))
# # plot(pred_lm2, y_test) + abline(0,1, col = 'red')
# # plot(pred_qrf2, y_test) + abline(0,1, col = 'red')
# # plot(pred_rf2, y_test) + abline(0,1, col = 'red')
# # plot(pred_xgbL2, y_test) + abline(0,1, col = 'red')
# # plot(pred_svmr2, y_test) + abline(0,1, col = 'red')
# # plot(pred_gbm2, y_test) + abline(0,1, col = 'red')
# plot(predict_ens2, y_test, xlab="Prediction",ylab="Actual") 
#   + abline(0,1, col = 'red')
```

### 12.7. Anomaly Detection

```{r}
# train_2 = filter(train, gridcode == 14 | 768) 
# q = train_2 |> select(Q) |> data.frame()
# train_2 = select(train_2, -Q)
# 
# iforest = isolationForest$new()
# train_2 = na.omit(train_2)
# train_2$gridcode = as.factor(train_2$gridcode)
# train_2[,2:7] = data.frame(scale(train_2[,2:7], TRUE, TRUE))
# 
# iforest$fit(train_2)
# train_2$pred = iforest$predict(train_2)
# train_2$label = as.factor(ifelse(train_2$pred$anomaly_score >=0.64, 
#                                    "anomaly", "normal"))
# 
# barplot(table(train_2$label),
# xlab = "Class",
# col = c("red","blue")
# )
# 
# qplot(train_2$pred$anomaly_score, q$Q, color = train_2$label)
# 
# filter(train_2, label=="anomaly")
```
### 12.8. Outlier Detection
```{r}
# getPalette = colorRampPalette(brewer.pal(9, "Set1"))
# 
# dt.valid <- na.omit(dt2) 
# qt <- quantile(dt.valid$Q, na.rm = TRUE)
# iqr <- qt[4]-qt[2]
# upper.bd <- qt[4]+iqr*1.5
# lower.bd <- qt[2]-iqr*1.5
# 
# ggplot(dt.valid, aes(y=Q, group=year,x=year)) + 
#   geom_boxplot() + 
#   geom_hline(yintercept = upper.bd) + 
#   geom_hline(yintercept = lower.bd) + 
#   ylab("Streamflow") + 
#   ggtitle("Streamflow Outliers (Grouped by Year)") 
```

